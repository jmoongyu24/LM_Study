{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self,hidden_size,vocab_size,seq_length,learning_rate):\n",
    "        #hyper parameters\n",
    "        self.hidden_size=hidden_size\n",
    "        self.vocab_size=vocab_size\n",
    "        self.seq_length=seq_length\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "        #model parameters\n",
    "        #np.random.uniform(low,high,size): low부터 high까지의 난수를 size 크기 만큼 생성\n",
    "        #1/vocab_size가 아닌 1./vocab_size인 이유: 1이 아닌 '1.'으로 명시적으로 실수형 연산 지정함\n",
    "        self.U=np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hiddem_sie, vocab_size))\n",
    "        self.V=np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W=np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        \n",
    "        #bias for hidden layer\n",
    "        self.b = np.zeros((hidden_size,1)) # bias for hidden layer.\n",
    "        #bias for output\n",
    "        self.c = np.zeros((vocab_size,1)) # bias for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Loss\n",
    "\n",
    "def forward(self, inputs, hprev):\n",
    "    xs, hs, os, yacp={}, {}, {}, {}\n",
    "    hs[-1]=np.copy(hprev)\n",
    "\n",
    "    for t in range(len(inputs)):\n",
    "        #xs[t]: 현재 시점 t의 입력 벡터(크기:(vocab_size,1))\n",
    "        xs[t]=zero_init(self.vocab_size, 1)\n",
    "        #ont-hot 인코딩\n",
    "        xs[t][inputs[t]]=1\n",
    "\n",
    "        #<hidden state>\n",
    "        #hs[t]: 현재 시점 t의 은닉 상태, np.tanh(): 하이퍼볼릭 탄젠트 함수(비선형 활성화 함수)\n",
    "        #hs[t-1]: 이전 시점 t-1의 은닉 상태(크기: (hidden_size,1))\n",
    "        #self.U: 입력 가중치 행렬(크기: (hidden_size, vocab_size))\n",
    "        #self.W: 은닉 상태 가중치 행렬(크기: (hidden_size, hidden_size))\n",
    "\n",
    "        #np.dot(self.U, xs[t]): 입력 xs[t]에 대한 선형 변환(입력 가중치 적용) / 현재 입력 xs[t]가 은닉 상태에 어떤 영향을 미치는지 계산\n",
    "        #np.dot(self.W, hs[t-1]): 이전 은닉 상태 hs[t-1]에 대한 선형 변환(은닉 상태 가중치 적용) / 이전 은닉 상태 hs[t-1]이 현재 은닉 상태에 어떤 영향을 미치는지 계산\n",
    "\n",
    "        hs[t]=np.tanh(np.dot(self.U, xs[t])+np.dot(self.W, hs[t-1])+self.b)\n",
    "\n",
    "        #<output> : 은닉 상태 hs[t]가 출력 os[t]로 바뀜\n",
    "        #os[t]: RNN의 출력층 값\n",
    "        #self.V: 출력 가중치 행렬(크기: (output_size, hidden_size))\n",
    "        os[t]=np.dot(self.V, hs[t])+self.c\n",
    "\n",
    "        #ycap[t]: 최종 확률 분포(예측값), 소프트맥스 함수로 0~1 사이 값으로 변환\n",
    "        ycap[t]=self.softmax(os[t])\n",
    "\n",
    "    return xs, hs, os, ycap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Loss\n",
    "\n",
    "def loss(self, ps, targets):\n",
    "    #loss for sequence\n",
    "    #cross entropy loss\n",
    "    \"\"\" \n",
    "    음의 로그 가능도, Negative Log-Likelihood 계산\n",
    "    1.모델이 예측한 확률 분포 ps[t]에서 정답 레이블 targets[t]의 확률값을 가져옴\n",
    "    2.확률값에 log 적용\n",
    "    3. 음수를 붙여 NLL 계산\n",
    "    4. 시퀀스 전체에 대해 손실 계산\n",
    "    \"\"\"\n",
    "    return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Pass\n",
    "\n",
    "\"\"\"\n",
    "xs: 입력 시퀀스, xs[t]는 t 시점의 입력 벡터\n",
    "hs: 은닉 상태, hs[t]는 t 시점의 은닉 상태\n",
    "ps: 출력 확률 분포, ps[t]는 t 시점의 소프트맥스 출력\n",
    "targets: 정답 레이블 시퀀스\n",
    "\"\"\"\n",
    "def backward(self, xs, hs, ps, targets):\n",
    "    #역전파: 역방향으로 기울기를 계산함\n",
    "    #dU, dW, dV: 가중치의 기울기 변화량을 저장하기 위해 사용, 동일한 크기, 0으로 초기화\n",
    "    dU, dW, dV=np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self,V)\n",
    "    #편향의 기울기 변화 저장\n",
    "    db, dc=np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    #다음 타임스텝에서 사용할 은닉 상태의 기울기, 0으로 초기화\n",
    "    dhnext=np.zeros_like(hs[0])\n",
    "\n",
    "    #시퀀스를 역순으로 순회(가장 마지막 시점부터 첫 번째 시점으로 진행)\n",
    "    for t in reversed(range(self.seq_length)):\n",
    "        #t 시점의 소프트맥스의 출력 저장\n",
    "        dy=np.copy(ps[t])\n",
    "        #targets[t]: 정답 클래스의 인덱스, 정답 클래스-1을 함으로써 소프트맥스의 기울기 구함\n",
    "        dy[targets[t]]-=1\n",
    "        #가중치 V의 기울기 업데이트\n",
    "        #dy: 출력층에서의 오차\n",
    "        dV+=np.dot(dy, hs[t].T)\n",
    "        dc+=dc\n",
    "\n",
    "        #출력층에서 전파된 기울기+다음 시점에서 전파된 기울기(이전 루프에서 계산된 값)\n",
    "        #dh: 현재 시점에서 다음 시점으로 전달할 기울기\n",
    "        dh=np.dot(self.V.T, dy)+dhnext # backprop into h\n",
    "        #활성화 함수로 탄젠트 하이퍼볼릭 함수 사용했으므로 tanh의 미분을 적용\n",
    "        #dhrec: 활성화 함수 통과한 후 현재 은닉 상태의 기울기\n",
    "        dhrec=(1-hs[t]*hs[t])*dh\n",
    "        db+=dhrec\n",
    "\n",
    "        #입력층 가중치 U의 기울기 계산, xs[t].T: 입력 시퀀스 xs[t]의 전치 행렬\n",
    "        #입력 데이터와 은닉 상태 기울기를 곱하여 가중치 변화량 구함함\n",
    "        dU+=np.dot(dhrec, xs[t].T)\n",
    "        dW+=np.dot(self.W.T, dhrec)\n",
    "\n",
    "    #기울기 클리핑, exploding gradient 방지, -5~5 사이 값으로 제한\n",
    "    for dparam in [dU, dW, dV, db, dc]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return dU, dW, dV, db, dc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Weights\n",
    "\n",
    "def update_model(self, dU, dW, dV, db, dc):\n",
    "    \"\"\"\n",
    "    param: 가중치 행렬 및 편향 벡터 - self.U, self.W, self.V, self.b, self.c\n",
    "    dparam: 해당 가중치의 기울기 - dU, dW, dV, db, dc\n",
    "    mem: Adagrad에서 사용하는 누적 기울기 제곱 - self.mU, self.mW, self.mV, self.mb, self.mc\n",
    "    param에 대해 기울기를 누적하고 Adagrad 업데이트 수행행\n",
    "    \"\"\"\n",
    "    for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "        #기울기 제곱 누적, 학습률 조정 역할\n",
    "        mem+=dparam*dparam\n",
    "        #adagrad update\n",
    "        param+=-self.learning_rate*dparam/np.sqrt(mem+1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Words\n",
    "\n",
    "\"\"\"\n",
    "data_reader: 문자열을 인덱스로 변환하는 객체, 학습된 데이터의 문자-인덱스 매핑 정보 담김\n",
    "start: 텍스트 생성을 시작할 초기 문자열\n",
    "n: 예측할 문자의 개수수\n",
    "\"\"\"\n",
    "def predict(self, data_reader, start, n):\n",
    "\n",
    "    #0으로 입력 벡터 초기화\n",
    "    \"\"\"\n",
    "    x: 원-핫 인코딩된 입력 벡터\n",
    "    self.vocab_size: 모델이 다룰 수 있는 전체 문자 개수\n",
    "    \"\"\"\n",
    "    x=zero_init(self.vocab_size, 1)\n",
    "    \n",
    "    #start: 문자열을 문자 단위로 리스트로 변환\n",
    "    #ixes는 예측된 문자들의 인덱스를 저장할 리스트\n",
    "    chars=[ch for ch in start]\n",
    "    ixes=[]\n",
    "    for i in range(len(chars)):\n",
    "        ix=data_reader.char_to_ix[chars[i]] #문자를 인덱스로 변환\n",
    "        x[ix]=1 #해당 위치를 1로 설정(원-핫 인코딩)\n",
    "        ixes.append(ix) # 변환된 문자 인덱스를 ixes 리스트에 저장장\n",
    "\n",
    "    #은닉 상태 초기화\n",
    "    h=np.zeros((self.hidden_size, 1))\n",
    "    #predict next n chars\n",
    "    for t in range(n):\n",
    "        h=np.tanh(np.dot(self.U, x)+np.dot(self.W,h)+self.b) #RNN의 은닉 상태 업데이트 공식\n",
    "        y=np.dot(self.V, h)+self.c #RNN의 출력 계산 공식식\n",
    "        p=np.exp(y)/np.sum(np.exp(y)) #확률 분포(소프트맥스 적용), y의 지수 값을 정규화하여 확률 계산산\n",
    "        \n",
    "        #확률 p를 기반으로 랜덤하게 문자 인덱스 샘플링\n",
    "        #p.ravel(): 벡터를 1차원 배열로 변환해 확률 분포로 활용\n",
    "        ix=np.random.choice(range(self.vocab_size),p=p.ravel())\n",
    "\n",
    "        #선택한 문자를 다음 입력으로 설정\n",
    "        x=zero_init(self.vocab_size, 1)\n",
    "        x[ix]=1\n",
    "\n",
    "        #ixes.append(ix): 새로 예측한 문자 인덱스를 저장\n",
    "        ixes.append(ix)\n",
    "        ixes.append(ix)\n",
    "    txt=''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/input-txt/input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#read text from the \"input.txt\" file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data_reader \u001b[38;5;241m=\u001b[39m \u001b[43mDataReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/input-txt/input.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39mdata_reader\u001b[38;5;241m.\u001b[39mvocab_size,seq_length\u001b[38;5;241m=\u001b[39mseq_length,learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# rnn.train(data_reader)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m, in \u001b[0;36mDataReader.__init__\u001b[1;34m(self, path, seq_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, seq_length):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#uncomment below , if you dont want to use any file for text reading and comment next 2 lines\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#find unique chars\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\Py3.9_AIPractice\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/input-txt/input.txt'"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"/kaggle/input/input-txt/input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "# rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.predict(data_reader, 'Alice', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(rows, cols):\n",
    "    return np.zeros((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = zero_init(self.vocab_size,1)\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = zero_init(self.vocab_size, 1)\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = zero_init(self.vocab_size,1)\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            threshold = 0.01\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "            while (smooth_loss > threshold):\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"/kaggle/input/input-txt/input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "# rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.predict(data_reader, 'Alice', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.9_AIPractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
